# Time series and outbreak detection { }
# 時系列とアウトブレイク検出{ }

<!-- ======================================================= -->
## Overview {  }
## 概観

This tab demonstrates the use of several packages for time series analysis. 
It primarily relies on packages from the [**tidyverts**](https://tidyverts.org/) 
family, but will also use the RECON [**trending**](https://github.com/reconhub/trending) 
package to fit models that are more appropriate for infectious disease epidemiology. 
このタブでは、時系列分析のためのいくつかのパッケージの使用方法を示します。

主に[**tidyverts**](https://tidyverts.org/)ファミリーのパッケージを使用しますが、RECON [**trending**](https://tidyverts.org/)も使用します。
ファミリーのパッケージに依存していますが、RECON [**trending**](https://github.com/reconhub/trending) 
パッケージを使用して、感染症の疫学に適したモデルをフィットさせます。

Note in the below example we use a dataset from the **surveillance** package 
on Campylobacter in Germany (see the [data chapter](https://epirhandbook.com/download-handbook-and-data.html), of the handbook for details). However, if you wanted to run the same code on a dataset
with multiple countries or other strata, then there is an example code template for this in the  [r4epis github repo](https://github.com/R4EPI/epitsa). 
以下の例では、ドイツのCampylobacterに関する**surveillance**パッケージのデータセットを使用しています（詳細はハンドブックの[data  chapter](https://epirhandbook.com/download-handbook-and-data.html)を参照してください）。
しかし、同じコードを複数の国や他の層を持つデータセットで実行したい場合は、[r4epis github repo](https://github.com/R4EPI/epitsa)にコードテンプレートの例がありますので、そちらを参照してください。

Topics covered include:  
トピックは以下の通りです。 


1.  Time series data 
2.  Descriptive analysis 
3.  Fitting regressions
4.  Relation of two time series 
5.  Outbreak detection
6.  Interrupted time series

1.  時系列データ  
2.  記述統計  
3.  回帰式のあてはめ  
4.  2つの時系列の関係  
5.  アウトブレイクの検出  
6.  遮断された時系列  

<!-- ======================================================= -->
## Preparation {  }
## 準備{ }

### Packages {.unnumbered}
# ###パッケージ {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics](https://epirhandbook.com/r-basics.html) for more information on R packages.  
このコードチャンクは、分析に必要なパッケージのロードを示しています。このハンドブックでは、**pacman**の`p_load()`を強調しています。これは、必要に応じてパッケージをインストールし、*使用するためにロードします。また、**base** Rの`library()`を使ってパッケージをロードすることもできます。Rパッケージの詳細については、[R basics (https://epirhandbook.com/r-basics.html)]のページを参照してください。

```{r load_packages}
pacman::p_load(rio,          # File import      # ファイルのインポート
               here,         # File locator     # ファイルのロケーター
               tidyverse,    # data management + ggplot2 graphics # データマネジメント + ggplot2の描画
               tsibble,      # handle time series datasets        # 時系列データセットの操作
               slider,       # for calculating moving averages    # 移動平均の計算のため
               imputeTS,     # for filling in missing values      # 欠損値のフィルタリングのため
               feasts,       # for time series decomposition and autocorrelation              # 時系列分解と自己相関のため
               forecast,     # fit sin and cosin terms to data (note: must load after feasts) # sinとcosの項をデータに当てはめる（注：feastsの後に読み込む必要がある）
               trending,     # fit and assess models              # モデルの当てはめと査定
               tmaptools,    # for getting geocoordinates (lon/lat) based on place names      # 地名から地理座標（lon/lat）を取得する機能
               ecmwfr,       # for interacting with copernicus sateliate CDS API              # copernicus sateliate CDS APIとのインタラクションのため
               stars,        # for reading in .nc (climate data) files                        # .nc（天候データ）ファイルの読み込みのため
               units,        # for defining units of measurement (climate data)               # 測定ユニット（天候データ）の定義のため
               yardstick,    # for looking at model accuracy                                  # 適切なモデルを探すため
               surveillance  # for aberration detection                                       # 異常検知のため
               )
``` 

### Load data {.unnumbered}

You can download all the data used in this handbook via the instructions in the [Download handbook and data] page.  
本ハンドブックで使用しているすべてのデータは、「ハンドブック・データのダウンロード」ページの手順でダウンロードできます。 

The example dataset used in this section is weekly counts of campylobacter cases reported in Germany between 2001 and 2011. <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/campylobacter_germany.xlsx' class='download-button'>
	You can click here to download<span> this data file (.xlsx).</span></a> 
ここでは、2001年から2011年にドイツで報告されたカンピロバクター症例の週次カウントをデータセットとして例示しています。<ahref='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/campylobacter_germany.xlsx' class='download-button'>
ここをクリックすると、<span>このデータファイル（.xlsx）をダウンロードすることができます。 </span> </span></span 

This dataset is a reduced version of the dataset available in the [**surveillance**](https://cran.r-project.org/web/packages/surveillance/) package. 
(for details load the surveillance package and see `?campyDE`)
このデータセットは、[**surveillance**](https://cran.r-project.org/web/packages/surveillance/)パッケージで利用できるデータセットの縮小版です。(詳細はサーベイランスパッケージを読み込んで、`?campyDE`を参照してください)

Import these data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).
これらのデータを **rio** パッケージの `import()` 関数を使ってインポートします（.xlsx, .csv, .rds など多くのファイルタイプを扱うことができます）。
```{r read_data_hide, echo=F}
# import the counts into R
# Rにcountsをインポート
counts <- rio::import(here::here("data", "time_series", "campylobacter_germany.xlsx"))
```

```{r read_data_show, eval=F}
# import the counts into R
# Rにcountsをインポート
counts <- rio::import("campylobacter_germany.xlsx")
```

The first 10 rows of the counts are displayed below.
最初の10行のカウントが以下に表示されます。
```{r inspect_data, message=FALSE, echo=F}
# display the counts data as a table
# countsをテーブルとして表示
DT::datatable(head(counts, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Clean data {.unnumbered}
### クリーンなデータ {.unnumbered}

The code below makes sure that the date column is in the appropriate format. 
For this tab we will be using the **tsibble** package and so the `yearweek` 
function will be used to create a calendar week variable. There are several other
ways of doing this (see the [Working with dates](https://epirhandbook.com/working-with-dates.html)
page for details), however for time series its best to keep within one framework (**tsibble**). 
以下のコードは、日付カラムが適切なフォーマットであることを確認します。
このタブでは、**tsibble**パッケージを使用しているので、`yearweek`関数を使用して暦週変数を作成しています。他にもいくつかの方法がありますが（詳細は[Working with dates](https://epirhandbook.com/working-with-dates.html)のページを参照してください）、時系列の場合は1つのフレームワーク（**tsibble**）に収めるのがベストです。

```{r clean_data}

## ensure the date column is in the appropriate format
## 日付カラムが適切なフォーマットであることを確認します
counts$date <- as.Date(counts$date)

## create a calendar week variable 
## カレンダーの週の変数を作成します
## fitting ISO definitons of weeks starting on a monday
## ISOの定義に準拠した月曜日から始まる週の定義にフィットさせます
counts <- counts %>% 
     mutate(epiweek = yearweek(date, week_start = 1))

```

### Download climate data {.unnumbered} 
### 天候データのダウンロード

In the *relation of two time series* section of this page, we will be comparing 
campylobacter case counts to climate data. 
このページの「2つの時系列の関連性」では、カンピロバクターの症例数と気候データを比較します。

Climate data for anywhere in the world can be downloaded from the EU's Copernicus 
Satellite. These are not exact measurements, but based on a model (similar to 
interpolation), however the benefit is global hourly coverage as well as forecasts.  
世界各地の気候データは、EUのコペルニクス衛星からダウンロードできます。これらは正確な測定値ではなく、モデルに基づいていますが（補間に似ています）、全世界を1時間ごとにカバーし、予測もできるという利点があります。 

You can download each of these climate data files from the [Download handbook and data] page.  
これらの各気候データファイルは、「ハンドブック・データのダウンロード」ページからダウンロードできます。 

For purposes of demonstration here, we will show R code to use the **ecmwfr** package to pull these data from the Copernicus 
climate data store. You will need to create a free account in order for this to 
work. The package website has a useful [walkthrough](https://github.com/bluegreen-labs/ecmwfr#use-copernicus-climate-data-store-cds)
of how to do this. Below is example code of how to go about doing this, once you 
have the appropriate API keys. You have to replace the X's below with your account
IDs. You will need to download one year of data at a time otherwise the server times-out.
ここではデモンストレーションとして、**ecmwfr**パッケージを使用してCopernicus気候データストアからこれらのデータを取得するためのRコードを紹介します。この機能を利用するには、無料のアカウントを作成する必要があります。パッケージのウェブサイトには、この方法に関する便利な[ウォークスルー](https://github.com/bluegreen-labs/ecmwfr#use-copernicus-climate-data-store-cds)があります。以下は、適切なAPIキーを取得した上で、これを実行する方法のサンプルコードです。下記のXをお客様のアカウントIDに置き換えていただく必要があります。一度に1年分のデータをダウンロードしないと、サーバーがタイムアウトしてしまうので注意が必要です。

If you are not sure of the coordinates for a location you want to download data 
for, you can use the **tmaptools** package to pull the coordinates off open street
maps. An alternative option is the [**photon**](https://github.com/rCarto/photon)
package, however this has not been released on to CRAN yet; the nice thing about 
**photon** is that it provides more contextual data for when there are several 
matches for your search.
データをダウンロードしたい場所の座標がわからない場合は、**tmaptools**パッケージを使って、オープンストリートマップから座標を引き出すことができます。別の選択肢として、[**photon**](https://github.com/rCarto/photon)パッケージがありますが、これはまだCRANにはリリースされていません。**photon**の良い点は、検索に複数のマッチがあった場合に、より多くのコンテキストデータを提供することです。

```{r weather_data, eval = FALSE}

## retrieve location coordinates
## 位置情報の取得
coords <- geocode_OSM("Germany", geometry = "point")

## pull together long/lats in format for ERA-5 querying (bounding box) 
## ERA-5のクエリに対応したフォーマットでlong/latsをまとめる(bounding box) 
## (as just want a single point can repeat coords)
## (1つの点が欲しいだけなので、座標を繰り返すことはできません)
request_coords <- str_glue_data(coords$coords, "{y}/{x}/{y}/{x}")



## Pulling data modelled from copernicus satellite (ERA-5 reanalysis)
## copernicus satellite (ERA-5 reanalysis)からモデル化したデータを引っ張ってくる。
## https://cds.climate.copernicus.eu/cdsapp#!/software/app-era5-explorer?tab=app
## https://github.com/bluegreen-labs/ecmwfr

## set up key for weather data 
## 天気データ用のキーを設定
wf_set_key(user = "XXXXX",
           key = "XXXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXX",
           service = "cds") 

## run for each year of interest (otherwise server times out)
## 対象となる年ごとに実行（そうしないとサーバーがタイムアウトする）
for (i in 2002:2011) {
  
  ## pull together a query 
  ## クエリを作成する
  ## see here for how to do: https://bluegreen-labs.github.io/ecmwfr/articles/cds_vignette.html#the-request-syntax
  ## 方法はこちらをご覧ください：https://bluegreen-labs.github.io/ecmwfr/articles/cds_vignette.html#the-request-syntax
  ## change request to a list using addin button above (python to list)
  ## 上記のAddinsボタンを使ってrequestをlistに変更（Pythonでlist化)
  ## Target is the name of the output file!!
  ## Targetは出力ファイルの名前です！！
  request <- request <- list(
    product_type = "reanalysis",
    format = "netcdf",
    variable = c("2m_temperature", "total_precipitation"),
    year = c(i),
    month = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12"),
    day = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12",
            "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
            "25", "26", "27", "28", "29", "30", "31"),
    time = c("00:00", "01:00", "02:00", "03:00", "04:00", "05:00", "06:00", "07:00",
             "08:00", "09:00", "10:00", "11:00", "12:00", "13:00", "14:00", "15:00",
             "16:00", "17:00", "18:00", "19:00", "20:00", "21:00", "22:00", "23:00"),
    area = request_coords,
    dataset_short_name = "reanalysis-era5-single-levels",
    target = paste0("germany_weather", i, ".nc")
  )
  
  ## download the file and store it in the current working directory
  ## ファイルをダウンロードして、現在の作業ディレクトリに保存します。
  file <- wf_request(user     = "XXXXX",  # user ID (for authentication) # ユーザーID（認証用）
                     request  = request,  # the request # request
                     transfer = TRUE,     # download the file # ファイルのダウンロード
                     path     = here::here("data", "Weather")) ## path to save the data # 保存データへのパス
  }

```

### Load climate data {.unnumbered}
### 天候データの読み込み {.unnumbered}

Whether you downloaded the climate data via our handbook, or used the code above, you now should have 10 years of ".nc" climate data files stored in the same folder on your computer.
気候データをハンドブックからダウンロードした場合も、上記のコードを使用した場合も、コンピュータの同じフォルダに10年分の「.nc」気候データファイルが保存されているはずです。

Use the code below to import these files into R with the **stars** package. 
以下のコードを使用して、これらのファイルを **stars** パッケージで R にインポートします。
```{r read_climate, warning = FALSE, message = FALSE}

## define path to weather folder 
## 天気フォルダへのパスの定義
file_paths <- list.files(
  here::here("data", "time_series", "weather"), # replace with your own file path # あなた自身のファイルパスとの置き換え
  full.names = TRUE)

## only keep those with the current name of interest
## 現在の興味ある名前のものだけを残す 
file_paths <- file_paths[str_detect(file_paths, "germany")]

## read in all the files as a stars object 
## 全てのファイルをstartsオブジェクトとして読み込み
data <- stars::read_stars(file_paths)
```

Once these files have been imported as the object `data`, we will convert them to a data frame.  
これらのファイルがオブジェクト `data` としてインポートされたら、データフレームに変換します。

```{r}
## change to a data frame 
## データフレームへの変更
temp_data <- as_tibble(data) %>% 
  ## add in variables and correct units
  ## 変数の追加と単位の修正
  mutate(
    ## create an calendar week variable 
    ## カレンダー週の変数の作成
    epiweek = tsibble::yearweek(time), 
    ## create a date variable (start of calendar week)
    ## 日付変数の作成（カレンダーの週の始まり）
    date = as.Date(epiweek),
    ## change temperature from kelvin to celsius
    ## 気温をケルビンから摂氏へ変更
    t2m = set_units(t2m, celsius), 
    ## change precipitation from metres to millimetres 
    ## 降水量をメートル単位からミリ単位へ変更
    tp  = set_units(tp, mm)) %>% 
  ## group by week (keep the date too though)
  ## 週でグループ化（日付も残す）
  group_by(epiweek, date) %>% 
  ## get the average per week
  ## 週平均を取得
  summarise(t2m = as.numeric(mean(t2m)), 
            tp = as.numeric(mean(tp)))

```




<!-- ======================================================= -->
## Time series data {  }
## 時系列データ{  }

There are a number of different packages for structuring and handling time series
data. As said, we will focus on the **tidyverts** family of packages and so will
use the **tsibble** package to define our time series object. Having a data set
defined as a time series object means it is much easier to structure our analysis. 
時系列データを構造化して扱うためのさまざまなパッケージがあります。前述の通り、ここでは**tidyverts**ファミリーのパッケージに焦点を当て、時系列オブジェクトを定義するために**tsibble**パッケージを使用します。データセットが時系列オブジェクトとして定義されていると、分析の構造化が非常に容易になります。

To do this we use the `tsibble()` function and specify the "index", i.e. the variable
specifying the time unit of interest. In our case this is the `epiweek` variable.
これには、`tsibble()`関数を使って、「インデックス」、つまり、対象となる時間単位を指定する変数を指定します。ここでは、`epiweek`という変数を使っています。

If we had a data set with weekly counts by province, for example, we would also 
be able to specify the grouping variable using the `key = ` argument. 
This would allow us to do analysis for each group. 
例えば、県別の週間カウントのデータセットがあったとすると、`key = ` という引数でグループ化変数を指定することもできます。  
これにより、グループごとの分析が可能となります。

```{r ts_object}

## define time series object 
## 時系列オブジェクトの定義
counts <- tsibble(counts, index = epiweek)

```

Looking at `class(counts)` tells you that on top of being a tidy data frame 
("tbl_df", "tbl", "data.frame"), it has the additional properties of a time series
data frame ("tbl_ts"). 
class(counts)`を見ると、整然としたデータフレーム("tbl_df", "tbl", "data.frame")であることに加えて、時系列データフレーム("tbl_ts")の追加特性を持っていることがわかります。

You can take a quick look at your data by using **ggplot2**. We see from the plot that
there is a clear seasonal pattern, and that there are no missings. However, there
seems to be an issue with reporting at the beginning of each year; cases drop 
in the last week of the year and then increase for the first week of the next year.
データは **ggplot2** を使って簡単に見ることができます。このプロットから、次のことがわかります。はっきりとした季節のパターンがあり、欠落はありません。しかし、このデータには しかし、年初の報告には問題があるようです。 年の最後の週に減少し、翌年の最初の週に増加しています。

```{r basic_plot}

## plot a line graph of cases by week
## 週ごとのケースを折れ線グラフにする
ggplot(counts, aes(x = epiweek, y = case)) + 
     geom_line()

```


<span style="color: red;">**_DANGER:_** Most datasets aren't as clean as this example. 
You will need to check for duplicates and missings as below. </span>
<span style="color: red;">**_DANGER:_** ほとんどのデータセットはこの例のようにきれいではありません。 重複や欠落を以下のようにチェックする必要があります。</span>。

<!-- ======================================================= -->
### Duplicates {.unnumbered}
### 重複{.unnumbered}

**tsibble** does not allow duplicate observations. So each row will need to be
unique, or unique within the group (`key` variable). 
The package has a few functions that help to identify duplicates. These include
`are_duplicated()` which gives you a TRUE/FALSE vector of whether the row is a 
duplicate, and `duplicates()` which gives you a data frame of the duplicated rows. 
**tsibble**では、観測値の重複を認めていません。そのため、各行が一意、またはグループ内で一意である必要があります（`key`変数）。
このパッケージには、重複を識別するのに役立つ関数がいくつかあります。以下はその例です。重複しているかどうかのTRUE/FALSEベクトルを与える`are_duplicated()`や，重複している行のデータフレームを与える`duplicates()`などがあります。

See the page on [De-duplication](https://epirhandbook.com/de-duplication.html)
for more details on how to select rows you want. 
必要な行の選択方法の詳細については[De-duplication](https://epirhandbook.com/de-duplication.html)のページを参照してください。

```{r duplicates, eval = FALSE}

## get a vector of TRUE/FALSE whether rows are duplicates
## 行が重複しているかどうかのTRUE/FALSEのベクトルを得る
are_duplicated(counts, index = epiweek) 

## get a data frame of any duplicated rows
## 重複している行のデータフレームを取得する
duplicates(counts, index = epiweek) 

```

<!-- ======================================================= -->
### Missings {.unnumbered}
### 欠損値{.unnumbered}

We saw from our brief inspection above that there are no missings, but we also 
saw there seems to be a problem with reporting delay around new year. 
One way to address this problem could be to set these values to missing and then 
to impute values. The simplest form of time series imputation is to draw
a straight line between the last non-missing and the next non-missing value. 
To do this we will use the **imputeTS** package function `na_interpolation()`. 
上記の簡単な検査では見逃しがないことを確認しましたが、新年の頃に報告が遅れるという問題があるようにも見えました。
この問題に対処する1つの方法は、これらの値を欠損に設定して値を入力することです。時系列入力の最も簡単な方法は、最後の非欠損値と次の非欠損値の間に直線を引くことです。
これを行うために， **imputeTS** パッケージの関数 `na_interpolation()` を使用します．

See the [Missing data](https://epirhandbook.com/missing-data.html) page for other options for imputation.  
代入の他のオプションについては、[Missing data](https://epirhandbook.com/missing-data.html)のページを参照してください。

Another alternative would be to calculate a moving average, to try and smooth
over these apparent reporting issues (see next section, and the page on [Moving averages](https://epirhandbook.com/moving-averages.html)). 
別の方法としては、移動平均を計算することでこれらの明らかな報告の問題をスムーズにすることができます（次のセクション、および[移動平均]のページ(https://epirhandbook.com/moving-averages.html)を参照）。

```{r missings}

## create a variable with missings instead of weeks with reporting issues
## 報告書の課題で週数ではなく欠勤数で変数を作成する
counts <- counts %>% 
     mutate(case_miss = if_else(
          ## if epiweek contains 52, 53, 1 or 2
          ## epiweekが52、53、1 または 2を含む場合
          str_detect(epiweek, "W51|W52|W53|W01|W02"), 
          ## then set to missing 
          ## 欠損値をセット
          NA_real_, 
          ## otherwise keep the value in case
          ## そう出ない場合、caseに値を保持
          case
     ))

## alternatively interpolate missings by linear trend 
## 直線的な傾向で欠損値を補う方法
## between two nearest adjacent points
## 隣接する2点の間
counts <- counts %>% 
  mutate(case_int = imputeTS::na_interpolation(case_miss)
         )

## to check what values have been imputed compared to the original
## ## 元の値と比較してどのような値がインピュートされたかを確認するために
ggplot_na_imputations(counts$case_miss, counts$case_int) + 
  ## make a traditional plot (with black axes and white background)
  ## 伝統的なプロット（軸が黒で背景が白）を作成します。
  theme_classic()

```




<!-- ======================================================= -->
## Descriptive analysis {  }
## 記述統計{  }


<!-- ======================================================= -->
### Moving averages {#timeseries_moving .unnumbered}
### 移動平均{#timeseries_moving .unnumbered}

If data is very noisy (counts jumping up and down) then it can be helpful to 
calculate a moving average. In the example below, for each week we calculate the 
average number of cases from the four previous weeks. This smooths the data, to 
make it more interpretable. In our case this does not really add much, so we will
stick to the interpolated data for further analysis. 
See the [Moving averages](https://epirhandbook.com/moving-averages.html) page for more detail. 

データが非常にノイジーな場合（カウントが跳ね上がったり下がったりする）は、移動平均を計算するのが有効です。
移動平均を計算することができます。以下の例では、各週ごとに、前の4週間の平均件数を計算します。
前の4週間の平均件数を計算します。これにより、データがスムーズになり 
より解釈しやすくなります。私たちの場合、これはあまり意味がありません。
さらなる分析のためには、補間されたデータにこだわります。
詳細は、[移動平均](https://epirhandbook.com/moving-averages.html)のページを参照してください。

```{r moving_averages}

## create a moving average variable (deals with missings)
## 移動平均変数の作成（欠損値の処理）
counts <- counts %>% 
     ## create the ma_4w variable 
     ## ma_4W 変数の作成
     ## slide over each row of the case variable
     ## ケース変数の各行をスライドさせる
     mutate(ma_4wk = slider::slide_dbl(case, 
                               ## for each row calculate the name
                               ## すべての業についてnameを計算する
                               ~ mean(.x, na.rm = TRUE),
                               ## use the four previous weeks
                               ## 4週前を使う
                               .before = 4))

## make a quick visualisation of the difference 
## 違いを一目で分かるように作成
ggplot(counts, aes(x = epiweek)) + 
     geom_line(aes(y = case)) + 
     geom_line(aes(y = ma_4wk), colour = "red")

```


<!-- ======================================================= -->
### Periodicity {.unnumbered}
### 周期性 {.unnumbered}

Below we define a custom function to create a periodogram. See the [Writing functions] page for information about how to write functions in R.
以下では、ピリオドグラムを作成するためのカスタム関数を定義します。Rでの関数の書き方については、「関数の書き方」のページを参照してください。

First, the function is defined. Its arguments include a dataset with a column `counts`, `start_week = ` which is the first week of the dataset, a number to indicate how many periods per year (e.g. 52, 12), and lastly the output style (see details in the code below).  
まず，この関数を定義します．この関数の引数には、`counts`列を持つデータセット、データセットの最初の週を表す`start_week = `、1年に何回の期間があるかを示す数字（例：52, 12）、そして最後に出力スタイルが含まれます（詳細は以下のコードを参照してください）。 

```{r periodogram}
## Function arguments
## 関数の引数
#####################
## x is a dataset
## xはデータセット
## counts is variable with count data or rates within x 
## countsはx内のカウントデータまたはレートの変数
## start_week is the first week in your dataset
## start_weekはあなたのデータセットの最初の州
## period is how many units in a year 
## periodは1年においてユニットがいくつあるか
## output is whether you want return spectral periodogram or the peak weeks
## outputはスペクトルのペリオドグラムを返すか、ピークの週数を返すかを指定します。
  ## "periodogram" or "weeks"
  ## ”periodogram" or "weeks"

# Define function
# 関数の定義
periodogram <- function(x, 
                        counts, 
                        start_week = c(2002, 1), 
                        period = 52, 
                        output = "weeks") {
  

    ## make sure is not a tsibble, filter to project and only keep columns of interest
    ## tsibbleでないことを確認し、プロジェクトにフィルターをかけ、関心のある列だけ残します。
    prepare_data <- dplyr::as_tibble(x)
    
    # prepare_data <- prepare_data[prepare_data[[strata]] == j, ]
    prepare_data <- dplyr::select(prepare_data, {{counts}})
    
    ## create an intermediate "zoo" time series to be able to use with spec.pgram
    ## spec.pgramで使用できるように、中間的な「zoo」時系列を作成します。
    zoo_cases <- zoo::zooreg(prepare_data, 
                             start = start_week, frequency = period)
    
    ## get a spectral periodogram not using fast fourier transform 
    ## 高速フーリエ変換を使わずにスペクトル・ピリオドグラムを得ることができます。
    periodo <- spec.pgram(zoo_cases, fast = FALSE, plot = FALSE)
    
    ## return the peak weeks 
    ## ピークの週を返します。
    periodo_weeks <- 1 / periodo$freq[order(-periodo$spec)] * period
    
    if (output == "weeks") {
      periodo_weeks
    } else {
      periodo
    }
    
}

## get spectral periodogram for extracting weeks with the highest frequencies 
## 最も多い頻度を持つ週を抽出するためのスペクトル・ピリオドグラムを取得します。
## (checking of seasonality) 
## （季節性の確認）
periodo <- periodogram(counts, 
                       case_int, 
                       start_week = c(2002, 1),
                       output = "periodogram")

## pull spectrum and frequence in to a dataframe for plotting
## スペクトルと周期をデータフレームに取り込み、プロットします。
periodo <- data.frame(periodo$freq, periodo$spec)

## plot a periodogram showing the most frequently occuring periodicity 
## 最も頻繁に発生する周期性を示すピリオドグラムを作成します。
ggplot(data = periodo, 
                aes(x = 1/(periodo.freq/52),  y = log(periodo.spec))) + 
  geom_line() + 
  labs(x = "Period (Weeks)", y = "Log(density)")


## get a vector weeks in ascending order
## 昇順で週のベクトルを取得します。
peak_weeks <- periodogram(counts, 
                          case_int, 
                          start_week = c(2002, 1), 
                          output = "weeks")

```

<span style="color: black;">**_NOTE:_** It is possible to use the above weeks to add them to sin and cosine terms, however we will use a function to generate these terms (see regression section below) </span>
<span style="color: black;">**_NOTE:_** 上の週を使ってsinとcosの項に追加することも可能ですが、ここではこれらの項を生成する関数を使用します（下記の回帰の項を参照）</span>。

<!-- ======================================================= -->
### Decomposition {.unnumbered}
### 分解 {.unnumbered}

Classical decomposition is used to break a time series down several parts, which
when taken together make up for the pattern you see. 
These different parts are:  

* The trend-cycle (the long-term direction of the data)  
* The seasonality (repeating patterns)  
* The random (what is left after removing trend and season)  

古典的分解は、時系列をいくつかの部分に分割するために使用され、それらの部分を組み合わせることで、目に見えるパターンを構成します。
これらの異なる部分とは

* トレンド・サイクル（データの長期的な方向性  
* 季節性（繰り返されるパターン  
* ランダム（トレンドと季節を取り除いた後に残るもの 

```{r decomposition, warning=F, message=F}

## decompose the counts dataset 
## counts データセットの分解
counts %>% 
  # using an additive classical decomposition model
  # 追加的な分解モデルを使用します。
  model(classical_decomposition(case_int, type = "additive")) %>% 
  ## extract the important information from the model
  ## モデルから重要な情報を抽出します。
  components() %>% 
  ## generate a plot 
  ## プロットを生成します。
  autoplot()

```

<!-- ======================================================= -->
### Autocorrelation {.unnumbered}
### 自己相関 {.unnumbered}

Autocorrelation tells you about the relation between the counts of each week 
and the weeks before it (called lags).
自己相関は、各週のカウント値とその前の週（ラグと呼ばれる）との関係を示します。

Using the `ACF()` function, we can produce a plot which shows us a number of lines 
for the relation at different lags. Where the lag is 0 (x = 0), this line would 
always be 1 as it shows the relation between an observation and itself (not shown here). 
The first line shown here (x = 1) shows the relation between each observation 
and the observation before it (lag of 1), the second shows the relation between 
each observation and the observation before last (lag of 2) and so on until lag of
52 which shows the relation between each observation and the observation from 1 
year (52 weeks before).  

`ACF()`関数を使って，異なるラグでの関係を示す複数の線を示すプロットを作成することができる。ラグが0(x = 0)の場合、この線は観測値とそれ自体の関係を示すため、常に1になります（ここでは示していません）。
ここで示されている最初の線（x = 1）は、各観測値とその前の観測値との関係（ラグ1）を示し、2番目の線は、各観測値と直前の観測値との関係（ラグ2）を示し、さらに1年後（52週前）の観測値との関係を示すラグ52まで続きます。

Using the `PACF()` function (for partial autocorrelation) shows the same type of relation 
but adjusted for all other weeks between. This is less informative for determining
periodicity. 
`PACF()`関数（部分自己相関）を使うと同じような関係が見られますが、他のすべての週の間で調整されています。これは、周期性を決定するための情報としては不十分です。

```{r autocorrelation}

## using the counts dataset
## countsデータセットを使用します。
counts %>% 
  ## calculate autocorrelation using a full years worth of lags
  ## 1年分のラグを使用して自己相関を計算します。
  ACF(case_int, lag_max = 52) %>% 
  ## show a plot
  ## プロットを表示します。
  autoplot()

## using the counts data set 
## countsデータセットを使用します。
counts %>% 
  ## calculate the partial autocorrelation using a full years worth of lags
  ## 1年分のラグを使用して部分自己相関を計算します。
  PACF(case_int, lag_max = 52) %>% 
  ## show a plot
  ## プロットを表示します。
  autoplot()

```

You can formally test the null hypothesis of independence in a time series (i.e. 
that it is not autocorrelated) using the Ljung-Box test (in the **stats** package). 
A significant p-value suggests that there is autocorrelation in the data.
Ljung-Box検定（**stats**パッケージ）を使用して、時系列の独立性の帰無仮説を正式に検定することができます（つまり、自己相関がない）。 
有意なp値は、データに自己相関があることを示唆します。

```{r ljung_box}

## test for independance 
## 独立性を検定します。
Box.test(counts$case_int, type = "Ljung-Box")

```


<!-- ======================================================= -->
## Fitting regressions {  }
## 回帰式の当てはめ {  }

It is possible to fit a large number of different regressions to a time series, 
however, here we will demonstrate how to fit a negative binomial regression - as 
this is often the most appropriate for counts data in infectious diseases. 
時系列には多くの異なる回帰をフィットさせることができますが、ここでは、負の二項回帰をフィットさせる方法を示します。 これは、感染症のカウントデータに最も適しているからです。

<!-- ======================================================= -->
### Fourier terms {.unnumbered}
### フーリエ項 {.unnumbered}

Fourier terms are the equivalent of sin and cosin curves. The difference is that 
these are fit based on finding the most appropriate combination of curves to explain
your data.
フーリエ項は、sinとcosinの曲線に相当します。違いは、データを説明するのに最も適した曲線の組み合わせを見つけてフィットさせることです。

If only fitting one fourier term, this would be the equivalent of fitting a sin 
and a cosin for your most frequently occurring lag seen in your periodogram (in our 
case 52 weeks). We use the `fourier()` function from the **forecast** package.
1つのフーリエ項をフィッティングするだけなら、ピリオドグラムに見られる最も頻繁に発生するラグ（ここでは52週）に対して、sinとcosinをフィッティングするのと同じことになります。ここでは、**forecast**パッケージの`fourier()`関数を使用しています。

In the below code we assign using the `$`, as `fourier()` returns two columns (one 
for sin one for cosin) and so these are added to the dataset as a list, called 
"fourier" - but this list can then be used as a normal variable in regression.
以下のコードでは、`$`を使って代入しています。これは、`fourier()`が2つの列（sinとcosin）を返すので、これらを「fourier」と呼ばれるリストとしてデータセットに追加しているためです。

```{r fourier}

## add in fourier terms using the epiweek and case_int variabless
## epiweekとcase_int変数を使ってフーリエ項を追加する
counts$fourier <- select(counts, epiweek, case_int) %>% 
  fourier(K = 1)
```

<!-- ======================================================= -->
### Negative binomial {.unnumbered}
### 負の二項 {.unnumbered}

It is possible to fit regressions using base **stats** or **MASS**
functions (e.g. `lm()`, `glm()` and `glm.nb()`). However we will be using those from 
the **trending** package, as this allows for calculating appropriate confidence
and prediction intervals (which are otherwise not available). 
The syntax is the same, and you specify an outcome variable then a tilde (~) 
and then add your various exposure variables of interest separated by a plus (+). 
ベースとなる **stats** や **MASS** 関数（例：`lm()`, `glm()`, `glm.nb()`）を用いて回帰をフィットさせることができます。しかし、ここでは**trending**パッケージのものを使用します。それは、適切な信頼区間と予測区間を計算できるからです（これは他の方法では利用できません）。構文は同じで、アウトカム変数を指定した後、チルダ(~)を入力し、プラス（+）で区切って興味のある様々な暴露変数を追加します。

The other difference is that we first define the model and then `fit()` it to the 
data. This is useful because it allows for comparing multiple different models 
with the same syntax. 
もう一つの違いは、まずモデルを定義し、それをデータに「fit()」することです。これは、同じ構文で複数の異なるモデルを比較できるという点で便利です。 

<span style="color: darkgreen;">**_TIP:_** If you wanted to use rates, rather than 
counts you could include the population variable as a logarithmic offset term, by adding 
`offset(log(population)`. You would then need to set population to be 1, before 
using `predict()` in order to produce a rate. </span>
<span style="color: darkgreen;">**_TIP:_** もし、数ではなく率を使いたい場合は、`offset(log(population)` を追加することで、対数オフセット項として population 変数を含めることができます。 レートを生成するには、`predict()`を使う前にpopulationを1に設定する必要があります。</span>

<span style="color: darkgreen;">**_TIP:_** For fitting more complex models such 
as ARIMA or prophet, see the [**fable**](https://fable.tidyverts.org/index.html) package.</span>
<span style="color: darkgreen;">**_TIP:_** ARIMAやprophetなどのより複雑なモデルのフィッティングについては、[**fable**](https://fable.tidyverts.org/index.html)パッケージをご参照ください。</span>。

```{r nb_reg, warning = FALSE}

## define the model you want to fit (negative binomial) 
## 当てはめたいモデル（負の二項回帰）の定義
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  ## 関心のあるアウトカムとしてケースの番号をセットします。
  case_int ~
    ## use epiweek to account for the trend
    ## トレンドを説明するためepiweekを使用します。
    epiweek +
    ## use the fourier terms to account for seasonality
    ## 季節性を説明するためフーリエ項を使用します。
    fourier)

## fit your model using the counts dataset
## countsデータセットを使用してモデルを当てはめます。
fitted_model <- trending::fit(model, counts)

## calculate confidence intervals and prediction intervals 
## 信頼区間と予測区間を計算します。
observed <- predict(fitted_model, simulate_pi = FALSE)

## plot your regression
## 回帰式をプロットします。
ggplot(data = observed, aes(x = epiweek)) + 
  ## add in a line for the model estimate
  ## モデル推定値の線を追加します。
  geom_line(aes(y = estimate),
            col = "Red") + 
  ## add in a band for the prediction intervals 
  ## 予測区間のバンドを追加します。
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add in a line for your observed case counts
  ## 観察されたケースの数を現す線を追加します。
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## make a traditional plot (with black axes and white background)
  ## 伝統的なプロット（軸が黒で背景が白）を作成します。
  theme_classic()


```

<!-- ======================================================= -->
### Residuals {.unnumbered}

To see how well our model fits the observed data we need to look at the residuals. 
The residuals are the difference between the observed counts and the counts 
estimated from the model. We could calculate this simply by using `case_int - estimate`, 
but the `residuals()` function extracts this directly from the regression for us.
我々のモデルが観測されたデータにどれだけフィットしているかを見るには、残差を見る必要があります。 残差は、観測されたカウントとモデルから推定されたカウントとの差です。これは、単純に`case_int - estimate`を使って計算することもできますが、`residuals()`関数は，回帰から直接残差を抽出してくれます。

What we see from the below, is that we are not explaining all of the variation 
that we could with the model. It might be that we should fit more fourier terms, 
and address the amplitude. However for this example we will leave it as is. 
The plots show that our model does worse in the peaks and troughs (when counts are
at their highest and lowest) and that it might be more likely to underestimate 
the observed counts. 
下の図からわかることは、モデルで説明できる変動のすべてを説明できていないということです。もっとフーリエ項をフィットさせて、振幅に対処すべきかもしれません。しかし、この例では、このままにしておきます。
このプロットは、我々のモデルが、山と谷（カウントが最大と最小の時 このプロットは、我々のモデルがピークとトラフ（カウントが最高と最低の時）で悪く、観測されたカウントをより過小評価する可能性があることを示しています。

```{r, warning=F, message=F}

## calculate the residuals 
## 残差を計算します。
observed <- observed %>% 
  mutate(resid = residuals(fitted_model$fitted_model, type = "response"))

## are the residuals fairly constant over time (if not: outbreaks? change in practice?)
## 残差は時間経過で概ね一定か？（そうでない場合： アウトブレイク？治療の変化？）
observed %>%
  ggplot(aes(x = epiweek, y = resid)) +
  geom_line() +
  geom_point() + 
  labs(x = "epiweek", y = "Residuals")

## is there autocorelation in the residuals (is there a pattern to the error?)  
## 残差に自己相関があるか（誤差にパターンがあるか？）
observed %>% 
  as_tsibble(index = epiweek) %>% 
  ACF(resid, lag_max = 52) %>% 
  autoplot()

## are residuals normally distributed (are under or over estimating?)  
## 残差が正規分布しているか（過小または過大推定か？）
observed %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 100) +
  geom_rug() +
  labs(y = "count") 
  
## compare observed counts to their residuals 
## 観測されたカウントとその残差を比較する
  ## should also be no pattern 
  ## パターンがないべき 
observed %>%
  ggplot(aes(x = estimate, y = resid)) +
  geom_point() +
  labs(x = "Fitted", y = "Residuals")

## formally test autocorrelation of the residuals
## 残差の自己相関を正式に検定します。
## H0 is that residuals are from a white-noise series (i.e. random)
## H0は残差がホワイトノイズ系列（つまりランダム）であるとします。
## test for independence 
## 独立性を検定します。
## if p value significant then non-random
## p値が有意であればランダムではありません。
Box.test(observed$resid, type = "Ljung-Box")

```

<!-- ======================================================= -->
## Relation of two time series {  }
## 2つの時系列の関係性 {  }

Here we look at using weather data (specifically the temperature) to explain 
campylobacter case counts. 
ここでは、気象データ（特に気温）を用いてカンピロバクターの症例数を説明する方法を見てみます。

<!-- ======================================================= -->
### Merging datasets {.unnumbered}
### データセットのマージ {.unnumbered}

We can join our datasets using the week variable. For more on merging see the 
handbook section on [joining](https://epirhandbook.com/joining-data.html).
weekという変数を使ってデータセットを結合することができます。結合の詳細については、ハンドブックの[結合](https://epirhandbook.com/joining-data.html)のセクションを参照してください。

```{r join}

## left join so that we only have the rows already existing in counts
## 左結合で、countsに既に存在する行だけを保持するようにします。
## drop the date variable from temp_data (otherwise is duplicated)
## temp_dataから、date変数を除外します（そうしないと重複します）。
counts <- left_join(counts, 
                    select(temp_data, -date),
                    by = "epiweek")

```

<!-- ======================================================= -->
### Descriptive analysis {.unnumbered}
### 記述統計 {.unnumbered}

First plot your data to see if there is any obvious relation. 
The plot below shows that there is a clear relation in the seasonality of the two
variables, and that temperature might peak a few weeks before the case number.
For more on pivoting data, see the handbook section on [pivoting data](https://epirhandbook.com/pivoting-data.html). 
まず、データをプロットして、明らかな関係があるかどうかを確認します。 下のプロットは、2つの変数の季節性に明確な関係があることを示しています。温度がケース番号の数週間前にピークに達することがあります。データのピボットについての詳細は、ハンドブックの[ピボットデータ](https://epirhandbook.com/pivoting-data.html)のセクションを参照してください。

```{r basic_plot_bivar}

counts %>% 
  ## keep the variables we are interested
  ## 関心のある変数を保持します。
  select(epiweek, case_int, t2m) %>% 
  ## change your data in to long format
  ## 縦型のフォーマットにデータを変更します。
  pivot_longer(
    ## use epiweek as your key
    ## epiweekをキーとして使用します。
    !epiweek,
    ## move column names to the new "measure" column
    ## 列名を新たな「measure」列に移動させます。
    names_to = "measure", 
    ## move cell values to the new "values" column
    ## セルの値を新たな「values」列に移動させます。
    values_to = "value") %>% 
  ## create a plot with the dataset above
  ## 上記のデータセットでプロットを作成します。
  ## plot epiweek on the x axis and values (counts/celsius) on the y 
  ## epiweekをX軸、値（カウント/摂氏）をY軸にプロットします。
  ggplot(aes(x = epiweek, y = value)) + 
    ## create a separate plot for temperate and case counts 
    ## 気温とケースの数について分離したプロットを作成します。
    ## let them set their own y-axes
    ## それらを書くY軸に配置します。
    facet_grid(measure ~ ., scales = "free_y") +
    ## plot both as a line
    ## 両方を線にしてプロットします。
    geom_line()

```

<!-- ======================================================= -->
### Lags and cross-correlation {.unnumbered}
### ラグと相互相関 {.unnumbered}

To formally test which weeks are most highly related between cases and temperature. 
We can use the cross-correlation function (`CCF()`) from the **feasts** package. 
You could also visualise (rather than using `arrange`) using the `autoplot()` function.
ケースと温度の間にどの週が最も高い関係にあるかを正式に検証するために  これには **feasts** パッケージの相互相関関数 (`CCF()`) が使えます。 また、（`arrange`ではなく）`autoplot()`関数を使って視覚化することもできます。

```{r cross_correlation, warning=FALSE}

counts %>% 
  ## calculate cross-correlation between interpolated counts and temperature
  ## 補完された和と気温の間の相互相関を計算します。
  CCF(case_int, t2m,
      ## set the maximum lag to be 52 weeks
      ## 最大のラグを52週としてセットします。
      lag_max = 52, 
      ## return the correlation coefficient 
      ## 相関係数を返します。
      type = "correlation") %>% 
  ## arange in decending order of the correlation coefficient 
  ## 相関係数を降順に並び替えします。
  ## show the most associated lags
  ## 最も関連するラグを表示します。
  arrange(-ccf) %>% 
  ## only show the top ten 
  ## 上位10のみ表示します。
  slice_head(n = 10)

```

We see from this that a lag of 4 weeks is most highly correlated, 
so we make a lagged temperature variable to include in our regression. 
このことから、4週間のラグが最も相関性が高いことがわかりますので、ラグ付きの温度変数を作って回帰に含めます。

<span style="color: red;">**_DANGER:_** Note that the first four weeks of our data
in the lagged temperature variable are missing (`NA`) - as there are not four 
weeks prior to get data from. In order to use this dataset with the **trending** 
`predict()` function, we need to use the the `simulate_pi = FALSE` argument within
`predict()` further down. If we did want to use the simulate option, then 
we have to drop these missings and store as a new data set by adding `drop_na(t2m_lag4)` 
to the code chunk below.</span> 
<span style="color: red;">**_DANGER:_** 遅延した温度変数のデータの最初の4週間が欠けている（`NA`）ことに注意してください - データを取得するための4週間前のデータがないからです。このデータセットを **トレンド** の `predict()` 関数で使用するためには、`predict()` の中の `simulate_pi = FALSE` 引数をさらに下の方で使用する必要があります。simulateオプションを使用したい場合は、以下のコードチャンクに`drop_na(t2m_lag4)`を追加することで、これらのミスを削除し、新しいデータセットとして保存しなければなりません。
 

```{r lag_tempvar}

counts <- counts %>% 
  ## create a new variable for temperature lagged by four weeks
  ## 4週でラグされた気温の変数を作成する。
  mutate(t2m_lag4 = lag(t2m, n = 4))

```


<!-- ======================================================= -->
### Negative binomial with two variables {.unnumbered}
### 2変数における負の二項分布 {.unnumbered}

We fit a negative binomial regression as done previously. This time we add the 
temperature variable lagged by four weeks. 
前述のように負の二項回帰を当てはめます。今回は、4週間遅れの 今回は、4週間遅れの温度変数を加えます。

<span style="color: orange;">**_CAUTION:_** Note the use of `simulate_pi = FALSE`
within the `predict()` argument. This is because the default behaviour of **trending** 
is to use the **ciTools** package to estimate a prediction interval. This does not 
work if there are `NA` counts, and also produces more granular intervals. 
See `?trending::predict.trending_model_fit` for details. </span>  
<span style="color: orange;">**_CAUTION:_** `predict()`の引数の中で`simulate_pi = FALSE`を使用していることに注意してください。これは、**trending**のデフォルトの動作として、**ciTools**パッケージを使用して予測区間を推定するためです。これは、`NA`カウントがある場合には機能せず、また、より詳細な間隔を生成します。詳細は `?trending::predict.trending_model_fit` を参照してください。</span> </span

```{r nb_reg_bivar, warning = FALSE}

## define the model you want to fit (negative binomial) 
## 当てはめたいモデル（負の二項分布）を定義します。
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  ## 関心のあるアウトカムとしてケースの数をセットします。
  case_int ~
    ## use epiweek to account for the trend
    ## トレンドの説明のためにepiweekを使います。
    epiweek +
    ## use the fourier terms to account for seasonality
    ## 季節性の説明のためにフーリエ項を使用します。
    fourier + 
    ## use the temperature lagged by four weeks 
    ## 4週間ラグされた気温を使います。
    t2m_lag4
    )

## fit your model using the counts dataset
## countsデータセットを使用してモデルの当てはめを行います。
fitted_model <- trending::fit(model, counts)

## calculate confidence intervals and prediction intervals 
## 信頼区間と予測区間を計算します。
observed <- predict(fitted_model, simulate_pi = FALSE)

```


To investigate the individual terms, we can pull the original negative binomial
regression out of the **trending** format using `get_model()` and pass this to the
**broom** package `tidy()` function to retrieve exponentiated estimates and associated
confidence intervals.
個々の項を調べるために、`get_model()`を使って**trending**フォーマットから元の負の二項回帰を取り出し、指数化された推定値と関連する信頼区間を取得するために、これを**broom**パッケージの`tidy()`関数に渡します。

What this shows us is that lagged temperature, after controlling for trend and seasonality, 
is similar to the case counts (estimate ~ 1) and significantly associated. 
This suggests that it might be a good variable for use in predicting future case
numbers (as climate forecasts are readily available). 
このことからわかるのは、トレンドと季節性を制御した後のラグ付き温度は、症例数（推定値～1）と類似しており、有意に関連していることがわかります。  
これは、将来の症例数を予測するのに適した変数であることを示唆しています（気候予測は容易に入手可能です）。

```{r results_nb_reg_bivar}

fitted_model %>% 
  ## extract original negative binomial regression
  ## 元の負の二項回帰を抽出します。
  get_model() %>% 
  ## get a tidy dataframe of result
  ## 結果についてのtidyなデータフレームを取得します。
  tidy(exponentiate = TRUE, 
       conf.int = TRUE)
```

A quick visual inspection of the model shows that it might do a better job of 
estimating the observed case counts. 
このモデルを目で見てみると、観測された症例数をより正確に推定することができるかもしれないことがわかります。

```{r plot_nb_reg_bivar, warning=F, message=F}

## plot your regression 
## 回帰式をプロットします。
ggplot(data = observed, aes(x = epiweek)) + 
  ## add in a line for the model estimate
  ## モデルの推定値の線を追加します。
  geom_line(aes(y = estimate),
            col = "Red") + 
  ## add in a band for the prediction intervals
  ## 予測区間のバンドを追加します。
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add in a line for your observed case counts
  ## 観測されたケースの数の線を追加します。
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## make a traditional plot (with black axes and white background)
  ## 伝統的なプロット（軸が黒で背景が白）を作成します。
  theme_classic()


```


#### Residuals {.unnumbered}
#### 残差 {.unnumbered}

We investigate the residuals again to see how well our model fits the observed data. 
The results and interpretation here are similar to those of the previous regression, 
so it may be more feasible to stick with the simpler model without temperature.

私たちのモデルが観測されたデータにどれだけフィットしているかを見るために、再び残差を調査します。
ここでの結果と解釈は前回の回帰のものと似ているので、温度なしのよりシンプルなモデルにこだわる方が実現性が高いかもしれません。

```{r}

## calculate the residuals 
## 残差を計算します。
observed <- observed %>% 
  mutate(resid = case_int - estimate)

## are the residuals fairly constant over time (if not: outbreaks? change in practice?)
## 残差は時間経過で概ね一定か？（そうでない場合： アウトブレイク？治療の変化？）
observed %>%
  ggplot(aes(x = epiweek, y = resid)) +
  geom_line() +
  geom_point() + 
  labs(x = "epiweek", y = "Residuals")

## is there autocorelation in the residuals (is there a pattern to the error?)  
## 残差に自己相関があるか（誤差にパターンがあるか？）
observed %>% 
  as_tsibble(index = epiweek) %>% 
  ACF(resid, lag_max = 52) %>% 
  autoplot()

## are residuals normally distributed (are under or over estimating?)  
## 残差が正規分布しているか（過小または過大推定か？） 
observed %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 100) +
  geom_rug() +
  labs(y = "count") 
  
## compare observed counts to their residuals 
## 観測されたカウントとその残差を比較する

  ## should also be no pattern 
  ## パターンがないべき 
observed %>%
  ggplot(aes(x = estimate, y = resid)) +
  geom_point() +
  labs(x = "Fitted", y = "Residuals")

## formally test autocorrelation of the residuals
## 残差の自己相関を正式に検定します。
## H0 is that residuals are from a white-noise series (i.e. random)
## H0は残差がホワイトノイズ系列（つまりランダム）であるとします。
## test for independence 
## 独立性を検定します。
## if p value significant then non-random
## p値が有意であればランダムではありません。
Box.test(observed$resid, type = "Ljung-Box")


## test for independence 
## 独立性を検定します。
## if p value significant then non-random
## p値が有意であればランダムではありません。

```

<!-- ======================================================= -->
## Outbreak detection {  }
## アウトブレイク {  }

We will demonstrate two (similar) methods of detecting outbreaks here. 
The first builds on the sections above. 
We use the **trending** package to fit regressions to previous years, and then
predict what we expect to see in the following year. If observed counts are above
what we expect, then it could suggest there is an outbreak. 
The second method is based on similar principles but uses the **surveillance** package,
which has a number of different algorithms for aberration detection.

ここでは、集団発生を検知する2つの（類似した）方法を紹介します。  
1つ目は、上記のセクションを基にしています。  
**trending**パッケージを使用して、過去の年に回帰を当てはめ、次の年に何が起こるかを予測します。観察された数が予測を上回っていれば、集団発生が発生していることを示唆しています。 
2つ目の方法は、同様の原理に基づいていますが、**surveillance**パッケージを使用します。このパッケージには、異常を検出するためのさまざまなアルゴリズムが含まれています。

<span style="color: orange;">**_CAUTION:_** Normally, you are interested in the current year (where you only know counts up to the present week). So in this example we are pretending to be in week 39 of 2011.</span>
<span style="color: orange;">**_CAUTION:_** 通常は、現在の年（現在の週までのカウントしかわからない場合）に関心があります。この例では、2011年の第39週にいると仮定しています。

<!-- ======================================================= -->
### **trending** package {.unnumbered}
### **trending** パッケージ {.unnumbered}

For this method we define a baseline (which should usually be about 5 years of data). 
We fit a regression to the baseline data, and then use that to predict the estimates
for the next year.
この方法では、ベースライン（通常は約5年分のデータ）を定義します。  
ベースラインのデータに回帰分析を行い、それをもとに次年度の推定値を予測します。

<!-- ======================================================= -->
#### Cut-off date { -}
#### 日付のカットオフ { -}

It is easier to define your dates in one place and then use these throughout the
rest of your code.
日付を一箇所で定義し、それを残りのコードで使用するのが簡単です。

Here we define a start date (when our observations started) and a cut-off date 
(the end of our baseline period - and when the period we want to predict for starts). 
~We also define how many weeks are in our year of interest (the one we are going to
be predicting)~.
We also define how many weeks are between our baseline cut-off and the end date 
that we are interested in predicting for.
ここでは、開始日（観測を開始した日）とカットオフ日（ベースライン期間の終了日であり、予測したい期間の開始日）を定義します。 
~また、対象年（予測する年）に何週あるかを定義します。~
また、ベースラインのカットオフと予測対象の終了日の間に何週あるかを定義します。予測したい終了日までの週数も定義します。

<span style="color: black;">**_NOTE:_** In this example we pretend to currently be at the end of September 2011 ("2011 W39").</span>
<span style="color: black;">**_NOTE:_** この例では、現在、2011年9月末（「2011W39」）にいることにしています。

```{r cut_off}

## define start date (when observations began)
## 開始日（いつ観測を開始したか）を定義します。
start_date <- min(counts$epiweek)

## define a cut-off week (end of baseline, start of prediction period)
## カットオフ週（ベースラインの最終、予測期間の開始）を定義します。
cut_off <- yearweek("2010-12-31")

## define the last date interested in (i.e. end of prediction)
## 関心のある最終日を定義します（例：予測の最後）
end_date <- yearweek("2011-12-31")

## find how many weeks in period (year) of interest
## 関心のある期間（年）における週の数を見つけます。
num_weeks <- as.numeric(end_date - cut_off)

```


<!-- ======================================================= -->
#### Add rows {.unnumbered}
#### 行の追加 {.unnumbered}

To be able to forecast in a tidyverse format, we need to have the right number 
of rows in our dataset, i.e. one row for each week up to the `end_date`defined above. 
The code below allows you to add these rows for by a grouping variable - for example
if we had multiple countries in one dataset, we could group by country and then 
add rows appropriately for each.
The `group_by_key()` function from **tsibble** allows us to do this grouping 
and then pass the grouped data to **dplyr** functions, `group_modify()` and 
`add_row()`. Then we specify the sequence of weeks between one after the maximum week 
currently available in the data and the end week. 

Tidyverse形式で予測するためには、データセットに適切な数の行が必要です。すなわち、上記で定義した`end_date`までの各週に1行ずつ必要です。 
以下のコードでは、グループ化変数によってこれらの行を追加することができます。例えば、1つのデータセットに複数の国がある場合、国ごとにグループ化して、それぞれに適切な行を追加することができます。
**tsibble**の`group_by_key()`関数により、このグループ化を行い、グループ化されたデータを**dplyr**の`group_modify()`および`add_row()`関数に渡すことができます。次に、現在データにある最大の週の1つ後から最終週までの週の順序を指定します。

```{r add_rows}

## add in missing weeks till end of year 
## 年末までの欠損週を追加します。
counts <- counts %>%
  ## group by the region
  ## 地域でグループ化します。
  group_by_key() %>%
  ## for each group add rows from the highest epiweek to the end of year
  ## 各グループに対して最も高いepiweekから年末までの行を追加します。
  group_modify(~add_row(.,
                        epiweek = seq(max(.$epiweek) + 1, 
                                      end_date,
                                      by = 1)))

```



<!-- ======================================================= -->
#### Fourier terms {.unnumbered}
#### フーリエ項 {.unnumbered}

We need to redefine our fourier terms - as we want to fit them to the baseline 
date only and then predict (extrapolate) those terms for the next year. 
To do this we need to combine two output lists from the `fourier()` function together; 
the first one is for the baseline data, and the second one predicts for the 
year of interest (by defining the `h` argument).
フーリエ項を再定義する必要があります。これは、基準日にのみフーリエ項をフィットさせ、翌年のフーリエ項を予測（外挿）したいからです。 
そのためには、`fourier()`関数からの2つのリストの出力を組み合わせる必要があります。1つ目はベースラインデータに対するもので、2つ目は（`h`引数を定義することで）対象となる年の予測です。

*N.b.* to bind rows we have to use `rbind()` (rather than tidyverse `bind_rows`) as
the fourier columns are a list (so not named individually). 
フーリエ列はリストなので（そのため個別に名前がついていない）、*N.b.*行を結合するには、（Tidyverseの`bind_rows`ではなく）`rbind()`を使用しなければなりません。

```{r fourier_terms_pred}


## define fourier terms (sincos) 
## フーリエ項（sincos）を定義します。
counts <- counts %>% 
  mutate(
    ## combine fourier terms for weeks prior to  and after 2010 cut-off date
    ## 2010年のカットオフ日前後の週のフーリエ項を組み合わせる
    ## (nb. 2011 fourier terms are predicted)
    ## （nb. フーリエ項は予測されます）
    fourier = rbind(
      ## get fourier terms for previous years
      ## 過去の年のフーリエ項を取得します。
      fourier(
        ## only keep the rows before 2011
        ## 2011年以前の行のみ保持します。
        filter(counts, 
               epiweek <= cut_off), 
        ## include one set of sin cos terms 
        ## サイン、コサインの項を1つのセットとして含めます。
        K = 1
        ), 
      ## predict the fourier terms for 2011 (using baseline data)
      ## 2011年におけるフーリエ項を予測します（ベースラインデータを用いて）。
      fourier(
        ## only keep the rows before 2011
        ## 2011年以前の行のみ保持します。
        filter(counts, 
               epiweek <= cut_off),
        ## include one set of sin cos terms
        ## サイン、コサインの項を1つのセットとして含めます。
        K = 1, 
        ## predict 52 weeks ahead
        ## 52週後を予測します。
        h = num_weeks
        )
      )
    )

```

<!-- ======================================================= -->
#### Split data and fit regression {.unnumbered}
#### データの分割と回帰式の当てはめ {.unnumbered}

We now have to split our dataset in to the baseline period and the prediction 
period. This is done using the **dplyr** `group_split()` function after `group_by()`, 
and will create a list with two data frames, one for before your cut-off and one 
for after.
次に、データセットをベースライン期間と予測期間に分割する必要があります。これは、`group_by()`の後に**dplyr**の`group_split()`関数を使って行い、カットオフ前とカットオフ後の2つのデータフレームを持つリストを作成します。

We then use the **purrr** package `pluck()` function to pull the datasets out of the
list (equivalent of using square brackets, e.g. `dat[[1]]`), and can then fit 
our model to the baseline data, and then use the `predict()` function for our data
of interest after the cut-off.
次に，**purrr** パッケージの `pluck()` 関数を使ってリストからデータセットを取り出し（角括弧を使った場合と同じです，例：`dat[[1]]`），ベースラインデータにモデルを適合させ，カットオフ後のデータに対して `predict()` 関数を使います．

See the page on [Iteration, loops, and lists] to learn more about **purrr**.
*purrr**については[反復、ループ、リスト]のページを参照してください。

<span style="color: orange;">**_CAUTION:_** Note the use of `simulate_pi = FALSE`
within the `predict()` argument. This is because the default behaviour of **trending** 
is to use the **ciTools** package to estimate a prediction interval. This does not 
work if there are `NA` counts, and also produces more granular intervals. 
See `?trending::predict.trending_model_fit` for details. </span>  
<span style="color: orange;">**_CAUTION:_** `predict()`の引数で`simulate_pi = FALSE`を使用していることに注意してください。これは、**trending**のデフォルトの動作として、**ciTools**のパッケージを使用して予測区間を推定するためです。これは、`NA`カウントがある場合には機能せず、また、より詳細な間隔を生成します。 詳細は `?trending::predict.trending_model_fit` を参照してください。</span>

```{r forecast_regression, warning = FALSE}
# split data for fitting and prediction
# 当てはめと予測のためにデータを分割します。

dat <- counts %>% 
  group_by(epiweek <= cut_off) %>%
  group_split()

## define the model you want to fit (negative binomial) 
## 当てはめたいモデル（負の二項回帰）を定義します。
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  ## 関心のあるアウトカムとしてケースの数をセットします。
  case_int ~
    ## use epiweek to account for the trend
    ## トレンドの説明のためにepiweekを使用します。
    epiweek +
    ## use the furier terms to account for seasonality
    ## 季節性の説明のためにフーリエ項を使用します。
    fourier
)

# define which data to use for fitting and which for predicting
## どのデータをあてはめに使用し、どのデータを予測に使用するかを定義します。
fitting_data <- pluck(dat, 2)
pred_data <- pluck(dat, 1) %>% 
  select(case_int, epiweek, fourier)

# fit model 
# モデルの当てはめを行います。
fitted_model <- trending::fit(model, fitting_data)

# get confint and estimates for fitted data
# 当てはめたデータのconfintと推定値を得ます。
observed <- fitted_model %>% 
  predict(simulate_pi = FALSE)

# forecast with data want to predict with 
# 予測したいデータで予測したいです。
forecasts <- fitted_model %>% 
  predict(pred_data, simulate_pi = FALSE)

## combine baseline and predicted datasets
## ベースラインと予測されたデータセットを結合します。
observed <- bind_rows(observed, forecasts)

```

As previously, we can visualise our model with **ggplot**. We highlight alerts with
red dots for observed counts above the 95% prediction interval. 
This time we also add a vertical line to label when the forecast starts.
前述のように、モデルを **ggplot** で可視化することができます。95%予測区間を超えて観測された数のアラートを赤いドットで強調しています。
今回は、予測開始時刻を示す縦線も加えています。

```{r forecast_plot}

## plot your regression 
## 回帰式をプロットします。
ggplot(data = observed, aes(x = epiweek)) + 
  ## add in a line for the model estimate
  ## モデル推定値についての線を追加します。
  geom_line(aes(y = estimate),
            col = "grey") + 
  ## add in a band for the prediction intervals 
  ## 予測区間のバンドを追加します。
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add in a line for your observed case counts
  ## 観測されたケースの数についての線を追加します。
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## plot in points for the observed counts above expected
  ## 期待値以上の観測値観測されたケースについての点をプロットします。
  geom_point(
    data = filter(observed, case_int > upper_pi), 
    aes(y = case_int), 
    colour = "red", 
    size = 2) + 
  ## add vertical line and label to show where forecasting started
  ## 予測開始の箇所を表示するために水平線とラベルを追加します。
  geom_vline(
           xintercept = as.Date(cut_off), 
           linetype = "dashed") + 
  annotate(geom = "text", 
           label = "Forecast", 
           x = cut_off, 
           y = max(observed$upper_pi) - 250, 
           angle = 90, 
           vjust = 1
           ) + 
  ## make a traditional plot (with black axes and white background)
  ## 伝統的なプロット（軸が黒で背景が白）を作成します。
  theme_classic()
```



<!-- ======================================================= -->
#### Prediction validation {.unnumbered}
#### 予測と妥当性検証 {.unnumbered}

Beyond inspecting residuals, it is important to investigate how good your model is
at predicting cases in the future. This gives you an idea of how reliable your 
threshold alerts are. 
残差を調べるだけでなく、モデルが将来のケースをどの程度予測できるかを調べることも重要です。これにより、閾値アラートの信頼性を知ることができます。

The traditional way of validating is to see how well you can predict the latest 
year before the present one (because you don't yet know the counts for the "current year"). 
For example in our data set we would use the data from 2002 to 2009 to predict 2010, 
and then see how accurate those predictions are. Then refit the model to include
2010 data and use that to predict 2011 counts.
従来の検証方法では、現在の年よりも前の最新の年をどれだけ予測できるかを確認していました（「現在の年」のカウント数がまだわからないため）。
例えば、今回のデータセットでは、2002年から2009年までのデータを使って2010年を予測し、その予測の正確さを確認します。その後、2010年のデータを含むようにモデルを修正し、それを使って2011年の数を予測します。

As can be seen in the figure below by *Hyndman et al* in ["Forecasting principles 
and practice"](https://otexts.com/fpp3/).
下の図は、Hyndman et al*による["Forecasting principles and practice"](https://otexts.com/fpp3/)に掲載されています。

![](`r "https://otexts.com/fpp3/fpp_files/figure-html/traintest-1.png"`)
*figure reproduced with permission from the authors*
*図は著者の許可を得て転載しています。*

The downside of this is that you are not using all the data available to you, and 
it is not the final model that you are using for prediction.
この場合のデメリットは、利用可能なすべてのデータを使用していないことと、予測に使用する最終モデルではないことです。

An alternative is to use a method called cross-validation. In this scenario you 
roll over all of the data available to fit multiple models to predict one year ahead. 
You use more and more data in each model, as seen in the figure below from the 
same [*Hyndman et al* text]((https://otexts.com/fpp3/). 
For example, the first model uses 2002 to predict 2003, the second uses 2002 and 
2003 to predict 2004, and so on.
もう一つの方法は、クロスバリデーションと呼ばれる方法です。このシナリオでは、利用可能なすべてのデータをロールオーバーして、1年先を予測するための複数のモデルに適合させます。
以下の図のように、それぞれのモデルにはより多くのデータを使用します。同じ[*Hyndman et al* text]((https://otexts.com/fpp3/). 
例えば、1つ目のモデルは2002年を使って2003年を予測し、2つ目は2002年と2003年を使って2004年を予測する、といった具合です。
![](`r "https://otexts.com/fpp2/fpp_files/figure-html/cv1-1.png"`)
*figure reproduced with permission from the authors*
*図は著者の許可を得て転載しています。*

In the below we use **purrr** package `map()` function to loop over each dataset. 
We then put estimates in one data set and merge with the original case counts, 
to use the **yardstick** package to compute measures of accuracy. 
We compute four measures including: Root mean squared error (RMSE), Mean absolute error	
(MAE), Mean absolute scaled error (MASE), Mean absolute percent error (MAPE).
以下では、 **purrr** パッケージの `map()` 関数を使って、各データセットをループさせます。
次に、推定値を1つのデータセットにまとめ、元の症例数とマージし、**yardstick**パッケージを使って精度の測定値を計算します。
以下の4つの指標を計算しました。平均平方根誤差（RMSE）、平均絶対誤差（MAE）、平均絶対尺度誤差（MASE）。平均絶対誤差(MAE)、平均絶対スケール誤差(MASE)、平均絶対パーセント誤差(MAPE)です。

<span style="color: orange;">**_CAUTION:_** Note the use of `simulate_pi = FALSE`
within the `predict()` argument. This is because the default behaviour of **trending** 
is to use the **ciTools** package to estimate a prediction interval. This does not 
work if there are `NA` counts, and also produces more granular intervals. 
See `?trending::predict.trending_model_fit` for details. </span>  
<span style="color: orange;">**_CAUTION:_** `predict()` の引数で `simulate_pi = FALSE` を使用していることに注意してください。これは **trending** のデフォルトの動作が **ciTools** パッケージを使用して予測区間を推定することだからです。
これは これは`NA`カウントがある場合には機能せず、また、より細かい間隔を生成します。 
詳細は `?trending::predict.trending_model_fit` を参照してください。</span>

```{r cross_validation, warning = FALSE}

## Cross validation: predicting week(s) ahead based on sliding window
## 交差確認法： スライディングウィンドウに基づいて1週間先を予測します。

## expand your data by rolling over in 52 week windows (before + after) 
## データを52週分（前+後）に分割して表示します。
## to predict 52 week ahead
## 52週先を予測すること
## (creates longer and longer chains of observations - keeps older data)
## (観察の連鎖がどんどん長くなり、古いデータが残ります。）

## define window want to roll over
## ロールオーバーしたい窓を定義します。
roll_window <- 52

## define weeks ahead want to predict 
## 何週間先を予測したいかを定義します。
weeks_ahead <- 52

## create a data set of repeating, increasingly long data
## 繰り返しの、長いデータセットを作成します。
## label each data set with a unique id
## 各データセットに一意のIDを付与します。
## only use cases before year of interest (i.e. 2011)
## 関心のある年（例： 2011年）の前のケースのみを使用します。
case_roll <- counts %>% 
  filter(epiweek < cut_off) %>% 
  ## only keep the week and case counts variables
  ## 週とケースの数の変数のみを保持します。
  select(epiweek, case_int) %>% 
    ## drop the last x observations 
    ## 最新のx観測値を落とします。
    ## depending on how many weeks ahead forecasting 
    ## 何週間先まで予測するかによって
    ## (otherwise will be an actual forecast to "unknown")
    ##（そうでない場合は""unknownへの実際の予測となります。） 
    slice(1:(n() - weeks_ahead)) %>%
    as_tsibble(index = epiweek) %>% 
    ## roll over each week in x after windows to create grouping ID 
    ## グループ化されたIDを作成するために、xの後のウィンドウで各週を検索します。
    ## depending on what rolling window specify
    ## ローリングウィンドウの指定に応じて
    stretch_tsibble(.init = roll_window, .step = 1) %>% 
  ## drop the first couple - as have no "before" cases
  ## 「以前」のケースがないため - 最初のカップルを削除します。
  filter(.id > roll_window)


## for each of the unique data sets run the code below
## それぞれのデータセットに対して、以下のコードを実行します。
forecasts <- purrr::map(unique(case_roll$.id), 
                        function(i) {
  
  ## only keep the current fold being fit
  ## 現在の折り返しをフィットさせるだけ
  mini_data <- filter(case_roll, .id == i) %>% 
    as_tibble()
  
  ## create an empty data set for forecasting on 
  ## 予測するための空のデータセットを作成します。
  forecast_data <- tibble(
    epiweek = seq(max(mini_data$epiweek) + 1,
                  max(mini_data$epiweek) + weeks_ahead,
                  by = 1),
    case_int = rep.int(NA, weeks_ahead),
    .id = rep.int(i, weeks_ahead)
  )
  
  ## add the forecast data to the original
  ## 予測データを元のデータに追加します。
  mini_data <- bind_rows(mini_data, forecast_data)
  
  ## define the cut off based on latest non missing count data
  ## 最新の非欠損カウントデータに基づいてカットオフを定義します。
  cv_cut_off <- mini_data %>% 
    ## only keep non-missing rows
    ## 欠損していない行のみを保持します。
    drop_na(case_int) %>% 
    ## get the latest week
    ## 最新の週を取得します。
    summarise(max(epiweek)) %>% 
    ## extract so is not in a dataframe
    ## 抽出したものはデータフレームには含まれません。
    pull()
  
  ## make mini_data back in to a tsibble
  ## mini_dataをtsibbleに戻します。
  mini_data <- tsibble(mini_data, index = epiweek)
  
  ## define fourier terms (sincos) 
  ## フーリエ項（sincos）を定義します。
  mini_data <- mini_data %>% 
    mutate(
    ## combine fourier terms for weeks prior to  and after cut-off date
    ## カットオフ日の前後の週のフーリエ項を組み合わせます。
    fourier = rbind(
      ## get fourier terms for previous years
      ## 以前の年のフーリエ項を取得します。
      forecast::fourier(
        ## only keep the rows before cut-off
        ## カットオフ以前の行のみを保持します。
        filter(mini_data, 
               epiweek <= cv_cut_off), 
        ## include one set of sin cos terms 
        ## サイン・コサイン項のセットを1つ含めます。
        K = 1
        ), 
      ## predict the fourier terms for following year (using baseline data)
      ## 後の年についてのフーリエ項を予測します（ベースラインデータを用いて）
      fourier(
        ## only keep the rows before cut-off
        ## カットオフ以前の行のみを保持します。
        filter(mini_data, 
               epiweek <= cv_cut_off),
        ## include one set of sin cos terms 
        ## サイン・コサインの項を1つ含めます。
        K = 1, 
        ## predict 52 weeks ahead
        ## 52週先を予測します。
        h = weeks_ahead
        )
      )
    )
  
  
  # split data for fitting and prediction
  ## 当てはめと予測のためにデータを分割します。
  dat <- mini_data %>% 
    group_by(epiweek <= cv_cut_off) %>%
    group_split()

  ## define the model you want to fit (negative binomial) 
  ## 当てはめたいモデルを定義します（負の二項回帰）
  model <- glm_nb_model(
    ## set number of cases as outcome of interest
    ## 関心のあるアウトカムとしてケースの数をセットします。
    case_int ~
      ## use epiweek to account for the trend
      ## 傾向を説明するためにepiweekを使います。
      epiweek +
      ## use the furier terms to account for seasonality
      ## 季節性を説明するためにフーリエ項を使用します。
      fourier
  )

  # define which data to use for fitting and which for predicting
  ## 度のデータを当てはめと予測に使用するかを定義します。
  fitting_data <- pluck(dat, 2)
  pred_data <- pluck(dat, 1)
  
  # fit model 
  # モデルの当てはめを行います。
  fitted_model <- trending::fit(model, fitting_data)
  
  # forecast with data want to predict with 
  # 予測したいデータを予想します。
  forecasts <- fitted_model %>% 
    predict(pred_data, simulate_pi = FALSE) %>% 
    ## only keep the week and the forecast estimate
    ## 週と予想する推定値のみを保持します。
    select(epiweek, estimate)
    
  }
  )

## make the list in to a data frame with all the forecasts
## リストを全ての予測を含むデータフレームにします。
forecasts <- bind_rows(forecasts)

## join the forecasts with the observed
## 予測値と観測値を結合します。
forecasts <- left_join(forecasts, 
                       select(counts, epiweek, case_int),
                       by = "epiweek")

## using {yardstick} compute metrics
## {yardstick}を用いて指標を計算します。
  ## RMSE: Root mean squared error
  ## RMSE： 平均二乗偏差
  ## MAE:  Mean absolute error	
  ## MAE： 平均絶対誤差
  ## MASE: Mean absolute scaled error
  ## MASE：平均絶対スケール誤差
  ## MAPE: Mean absolute percent error
  ## MAPE： 平均z接待パーセント誤差
model_metrics <- bind_rows(
  ## in your forcasted dataset compare the observed to the predicted
  ## 予測されたデータセット内の観測値と推定値を比較
  rmse(forecasts, case_int, estimate), 
  mae( forecasts, case_int, estimate),
  mase(forecasts, case_int, estimate),
  mape(forecasts, case_int, estimate),
  ) %>% 
  ## only keep the metric type and its output
  ## 指標タイプと結果のみを保持します。
  select(Metric  = .metric, 
         Measure = .estimate) %>% 
  ## make in to wide format so can bind rows after
  ## 後で行を結合できるようにワイドフォーマットで作成します。
  pivot_wider(names_from = Metric, values_from = Measure)

## return model metrics 
## モデルの指標を返します。
model_metrics

```


<!-- ======================================================= -->
### **surveillance** package {.unnumbered}
### **surveillance** パッケージ {.unnumbered}

In this section we use the **surveillance** package to create alert thresholds 
based on outbreak detection algorithms. There are several different methods 
available in the package, however we will focus on two options here. 
For details, see these papers on the [application](https://cran.r-project.org/web/packages/surveillance/vignettes/monitoringCounts.pdf)
and [theory](https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf)
of the alogirthms used. 
このセクションでは、アウトブレイク検出アルゴリズムに基づいて警告のしきい値を作成するために**surveillance**パッケージを使用します。このパッケージにはいくつかの異なる手法がありますが、ここでは2つのオプションに焦点を当てます。

詳細については、使用したアルゴリズムの[application](https://cran.r-project.org/web/packages/surveillance/vignettes/monitoringCounts.pdf)と[theory](https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf)に関するこれらの論文を参照してください。

The first option uses the improved Farrington method. This fits a negative 
binomial glm (including trend) and down-weights past outbreaks (outliers) to 
create a threshold level.
最初のオプションでは、改良型のFarrington法を使用します。これは、負の二項一般化線形モデル（トレンドを含む）に適合し、過去の発生（外れ値）をダウンウェイトして、閾値を作成します。

The second option use the glrnb method. This also fits a negative binomial glm 
but includes trend and fourier terms (so is favoured here). The regression is used
to calculate the "control mean" (~fitted values) - it then uses a computed 
generalized likelihood ratio statistic to assess if there is shift in the mean 
for each week. Note that the threshold for each week takes in to account previous
weeks so if there is a sustained shift an alarm will be triggered. 
(Also note that after each alarm the algorithm is reset)
2つ目のオプションはglrnbメソッドを使用します。これも負の二項GLMにフィットしますが、トレンドとフーリエ項を含んでいますので、ここでは好まれます。この回帰は、「対照平均」（～適合値）を計算するために使用されます。そして、各週の平均にシフトがあるかどうかを評価するために、計算された一般化された尤度比統計を使用します。各週のしきい値は、前の週を考慮していることに注意してください。各週の閾値は過去の週を考慮しているため、持続的なシフトがある場合はアラームが作動します。 (また、各アラームの後、アルゴリズムはリセットされることにも注意してください。）

In order to work with the **surveillance** package, we first need to define a 
"surveillance time series" object (using the `sts()` function) to fit within the 
framework. 
 **surveillance** パッケージを使用するためには、まず、フレームワークに適合する "surveillance time series" オブジェクトを定義する必要があります（`sts()`関数を使用）。

```{r surveillance_obj}

## define surveillance time series object
## surveillance time series オブジェクトを定義します。
## nb. you can include a denominator with the population object (see ?sts)
## nb. 母集団オブジェクトに分母を含めることができます。（?stsを参照ください。）
counts_sts <- sts(observed = counts$case_int[!is.na(counts$case_int)],
                  start = c(
                    ## subset to only keep the year from start_date 
                    ## start_date から年のみを保持するサブセット
                    as.numeric(str_sub(start_date, 1, 4)), 
                    ## subset to only keep the week from start_date
                    ## start_date から週のみを保持するサブセット
                    as.numeric(str_sub(start_date, 7, 8))), 
                  ## define the type of data (in this case weekly)
                  ## データの種類を定義します。（このケースでは週次）
                  freq = 52)

## define the week range that you want to include (ie. prediction period)
## 含めたい週の範囲を定義します（例：予測期間）
## nb. the sts object only counts observations without assigning a week or 
## nb. sts オブジェクトは、週を指定せずにオブザベーションをカウントするだけです。
## year identifier to them - so we use our data to define the appropriate observations
## sts オブジェクトは、週や ## 年の識別子を割り当てずに観測値をカウントしているだけなので、我々のデータを使って適切な観測値を定義します。
weekrange <- cut_off - start_date

```

<!-- ======================================================= -->
#### Farrington method {.unnumbered}
#### Farrington 法 {.unnumbered}

We then define each of our parameters for the Farrington method in a `list`. 
Then we run the algorithm using `farringtonFlexible()` and then we can extract the 
threshold for an alert using `farringtonmethod@upperbound`to include this in our 
dataset. It is also possible to extract a TRUE/FALSE for each week if it triggered 
an alert (was above the threshold) using `farringtonmethod@alarm`. 
次に、Farringtonメソッドの各パラメータを `list` で定義します。
そして、`farringtonFlexible()`を使ってアルゴリズムを実行し、`farringtonmethod@upperbound`を使ってアラートの閾値を抽出し、データセットに含めることができます。また、`farringtonmethod@alarm`を使って、各週にアラートが発生した(閾値を超えた)場合のTRUE / FALSEを抽出することもできます。

```{r farrington}

## define control
## コントロール群を定義します。
ctrl <- list(
  ## define what time period that want threshold for (i.e. 2011)
  ## 閾値が必要な期間を定義します。（例： 2011年）
  range = which(counts_sts@epoch > weekrange),
  b = 9, ## how many years backwards for baseline ## ベースラインを何年前にするか。
  w = 2, ## rolling window size in weeks ## ローリングウィンドウのサイズ
  weightsThreshold = 2.58, ## reweighting past outbreaks (improved noufaily methodi - original suggests 1) ## 過去の発生事例への再重みづけ（改良型のNoufaily法 - オリジナルの提案1）
  ## pastWeeksNotIncluded = 3, ## use all weeks available (noufaily suggests drop 26) ## pastWeeksNotIncluded = 3, ## 利用可能なすべての週を使う (Noufaily は 26 を落とすことを提案)
  trend = TRUE,
  pThresholdTrend = 1, ## 0.05 normally, however 1 is advised in the improved method (i.e. always keep) ## 通常は0.05ですが、改良された方法では1が推奨されます（つまり、常にキープされます）。
  thresholdMethod = "nbPlugin",
  populationOffset = TRUE
  )

## apply farrington flexible method
## farrington の柔軟な方法を適用します。
farringtonmethod <- farringtonFlexible(counts_sts, ctrl)

## create a new variable in the original dataset called threshold
## オリジナルのデータセットに閾値という新たな変数を作成します。
## containing the upper bound from farrington 
## farringtonからの上界を含みます。
## nb. this is only for the weeks in 2011 (so need to subset rows)
## nb. これは2011年の週のみを対象としています（そのため、行をサブセット化する必要があります）。
counts[which(counts$epiweek >= cut_off & 
               !is.na(counts$case_int)),
              "threshold"] <- farringtonmethod@upperbound
```

We can then visualise the results in ggplot as done previously. 
前に実施したように、結果をggplotによって可視化できます。

```{r plot_farrington, warning=F, message=F}

ggplot(counts, aes(x = epiweek)) + 
  ## add in observed case counts as a line
  ## 観測されたケースを線として追加します。
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## add in upper bound of aberration algorithm
  ## 収差アルゴリズムの上界を追加します。
  geom_line(aes(y = threshold, colour = "Alert threshold"), 
            linetype = "dashed", 
            size = 1.5) +
  ## define colours
  ## 色を定義します。
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Alert threshold" = "red")) + 
  ## make a traditional plot (with black axes and white background)
  ## 伝統的なプロット（軸が黒で背景が白）を作成します。
  theme_classic() + 
  ## remove title of legend 
  ## legendのタイトルを除外します。
  theme(legend.title = element_blank())

```

<!-- ======================================================= -->
#### GLRNB method {.unnumbered}
#### GLRNB 法 {.unnumbered}

Similarly for the GLRNB method we define each of our parameters for the in a `list`, 
then fit the algorithm and extract the upper bounds.
GLRNB法の場合も同様に，各パラメータを「リスト」で定義します。そして，アルゴリズムをフィットさせ，上界を抽出します．

<span style="color: orange;">**_CAUTION:_** This method uses "brute force" (similar to bootstrapping) for calculating thresholds, so can take a long time!</span>
<span style="color: orange;">**_CAUTION:_** この方法は、閾値の計算に「ブルートフォース」（ブートストラップ法に似た方法）を使用するため、長い時間がかかることがあります！</span>。

See the [GLRNB vignette](https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf) 
for details. 
詳細は[GLRNB vignette](https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf) を参照してください。

```{r glrnb, warning = FALSE, message = FALSE}

## define control options
## 統制するオプションを定義します。
ctrl <- list(
  ## define what time period that want threshold for (i.e. 2011)
  ## 閾値が必要な期間を定義します。 (例：2011年)
  range = which(counts_sts@epoch > weekrange),
  mu0 = list(S = 1,    ## number of fourier terms (harmonics) to include ## ## 含めるフーリエ項（ハーモニクス）の数
  trend = TRUE,   ## whether to include trend or not ## トレンドを含めるかどうか
  refit = FALSE), ## whether to refit model after each alarm ## それぞれの警告の後、モデルを再度当てはめるかどうか
  ## cARL = threshold for GLR statistic (arbitrary)
  ## cARL = GLR 統計の閾値（任意）
     ## 3 ~ middle ground for minimising false positives
     ## ## 3 ~ 偽陽性を最小限に抑えるための中間地点
     ## 1 fits to the 99%PI of glm.nb - with changes after peaks (threshold lowered for alert)
     ## ## 1 glm.nbの99%PIにフィット - ピーク後の変化あり（警告のために閾値を下げます）
   c.ARL = 2,
   # theta = log(1.5), ## equates to a 50% increase in cases in an outbreak
   # theta = log(1.5), ## アウトブレイクにおける症例数の50%増加に等しい
   ret = "cases"     ## return threshold upperbound as case counts ## ケースの数として閾値の上界を返します。
  )

## apply the glrnb method
## glrnb法を適用します。
glrnbmethod <- glrnb(counts_sts, control = ctrl, verbose = FALSE)

## create a new variable in the original dataset called threshold
## 元のデータセットに閾値と名付けた新たな変数をセットします。
## containing the upper bound from glrnb 
## glrnbの上界を含みます。
## nb. this is only for the weeks in 2011 (so need to subset rows)
## nb. これは2011年の週のみを対象としています（そのため、行をサブセット化する必要があります）。
counts[which(counts$epiweek >= cut_off & 
               !is.na(counts$case_int)),
              "threshold_glrnb"] <- glrnbmethod@upperbound

```

Visualise the outputs as previously. 

```{r plot_glrnb, message=F, warning=F}

ggplot(counts, aes(x = epiweek)) + 
  ## add in observed case counts as a line
  ## 観測されたケースの数を線として追加します。
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## add in upper bound of aberration algorithm
  ## aberration アルゴリズムの上界を追加します。
  geom_line(aes(y = threshold_glrnb, colour = "Alert threshold"), 
            linetype = "dashed", 
            size = 1.5) +
  ## define colours
  ## 色を定義します。
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Alert threshold" = "red")) + 
  ## make a traditional plot (with black axes and white background)
  ## 伝統的なプロット（軸が黒で背景が白）を作成します。
  theme_classic() + 
  ## remove title of legend 
  ## タイトルのレジェンドを除外します。
  theme(legend.title = element_blank())

```

<!-- ======================================================= -->
## Interrupted timeseries {  }
## 中断された時系列 {  }

Interrupted timeseries (also called segmented regression or intervention analysis), 
is often used in assessing the impact of vaccines on the incidence of disease. 
But it can be used for assessing impact of a wide range of interventions or introductions. 
For example changes in hospital procedures or the introduction of a new disease 
strain to a population. 
In this example we will pretend that a new strain of Campylobacter was introduced
to Germany at the end of 2008, and see if that affects the number of cases. 
We will use negative binomial regression again. The regression this time will be 
split in to two parts, one before the intervention (or introduction of new strain here) 
and one after (the pre and post-periods). This allows us to calculate an incidence rate ratio comparing the
two time periods. Explaining the equation might make this clearer (if not then just
ignore!). 

中断された時系列（セグメント回帰や介入分析とも呼ばれる。） は、病気の発生率に対するワクチンの影響を評価する際によく使用されます。 しかし、これは広範囲の介入や導入の影響を評価するために使用することができます。
例えば、病院の手順の変更や、集団への新しい病気の導入などです。
今回の例では、2008年末にドイツでカンピロバクターの新しい株が導入されたと仮定して、それが患者数に影響を与えるかどうかを調べます。
今回も負の二項回帰を使用します。今回の回帰では、介入前（または新菌株の導入）と介入後（前後の期間）の2つの部分に分けます。これにより、2つの期間を比較した罹患率比を算出することができます。式を説明するとわかりやすいかもしれません（そうでなければ無視してください！）。

The negative binomial regression can be defined as follows: 
負の2項回帰は次の通り定義できます。

$$\log(Y_t)= β_0 + β_1 \times t+ β_2 \times δ(t-t_0) + β_3\times(t-t_0 )^+ + log(pop_t) + e_t$$

Where:
$Y_t$is the number of cases observed at time $t$  
$pop_t$ is the population size in 100,000s at time $t$ (not used here)  
$t_0$ is the last year of the of the pre-period (including transition time if any)  
$δ(x$ is the indicator function (it is 0 if x≤0 and 1 if x>0)  
$(x)^+$ is the cut off operator (it is x if x>0 and 0 otherwise)  
$e_t$ denotes the residual 
Additional terms trend and season can be added as needed. 

ここで
$Y_t$ は $t$ 時点で観測された症例数  
$pop_t$ は、 $t$ 時点での10万人単位の人口規模（ここでは使用しません）。 
$t_0$ は前期間の最後の年（移行期間がある場合はその期間も含みます）。 
$δ(x)$ は指標関数（x≤0なら0、x>0なら1）。 
$(x)^+$ はカットオフ演算子（x>0ならx、それ以外は0）。 
$e_t$ は残差を表します。
必要に応じてトレンドや季節などの項を追加することができます。

$β_2 \times δ(t-t_0) + β_3\times(t-t_0 )^+$ is the generalised linear 
part of the post-period and is zero in the pre-period. 
This means that the $β_2$ and $β_3$ estimates are the effects of the intervention.

$β_2 \times δ(t-t_0) + β_3 \times(t-t_0 )^+$ は、後の期間の一般化された線形部分であり、前期はゼロです。これは、 $β_2$ と $β_3$ の推定値は介入の効果であることを意味します。

We need to re-calculate the fourier terms without forecasting here, as we will use
all the data available to us (i.e. retrospectively). Additionally we need to calculate
the extra terms needed for the regression.
ここでは、利用可能なすべてのデータを使用するため、予測を行わずにフーリエ項を再計算する必要があります（つまり、過去にさかのぼって計算することになります）。さらに、回帰に必要な追加の項を計算する必要があります。

```{r define_terms_interrupted}

## add in fourier terms using the epiweek and case_int variabless
## epiweek と case_int variabless を使用してフーリエ項を追加します。
counts$fourier <- select(counts, epiweek, case_int) %>% 
  as_tsibble(index = epiweek) %>% 
  fourier(K = 1)

## define intervention week 
## 介入の州を定義します。
intervention_week <- yearweek("2008-12-31")

## define variables for regression 
## 回帰分析のための変数を定義します。
counts <- counts %>% 
  mutate(
    ## corresponds to t in the formula
    ## 公式における t に対応させます。
      ## count of weeks (could probably also just use straight epiweeks var)
      ## 週数 (epiweeksをそのまま使うこともできるでしょう)
    # linear = row_number(epiweek), 
    # 線形 = 行の数（epiweek）
    ## corresponds to delta(t-t0) in the formula
    ## 公式におけるデルタ（t-t0）に対応させます。
      ## pre or post intervention period
      ## 前と後の介入期間
    intervention = as.numeric(epiweek >= intervention_week), 
    ## corresponds to (t-t0)^+ in the formula
    ## 公式に置ける（(t-t0)^+ に対応させます。
      ## count of weeks post intervention
      ## 介入後の週の数
      ## (choose the larger number between 0 and whatever comes from calculation)
      ## ## (0から計算で出てくる数字のうち、大きい方を選びます。)
    time_post = pmax(0, epiweek - intervention_week + 1))

```

We then use these terms to fit a negative binomial regression, and produce a 
table with percentage change. What this example shows is that there was no 
significant change. 
次に、これらの項を使って負の二項回帰をあてはめ、変化率の表を作成します。この例では、有意な変化はありませんでした。

<span style="color: orange;">**_CAUTION:_** Note the use of `simulate_pi = FALSE`
within the `predict()` argument. This is because the default behaviour of **trending** 
is to use the **ciTools** package to estimate a prediction interval. This does not 
work if there are `NA` counts, and also produces more granular intervals. 
See `?trending::predict.trending_model_fit` for details. </span>  
<span style="color: orange;">**_CAUTION:_** `predict()` の引数で `simulate_pi = FALSE` を使用していることに注意してください。これは **trending** のデフォルトの動作が **ciTools** パッケージを使用して予測区間を推定することだからです。
これは これは`NA`カウントがある場合には機能せず、また、より細かい間隔を生成します。詳細は `?trending::predict.trending_model_fit` を参照してください。</span>

```{r interrupted_regression, warning = FALSE}


## define the model you want to fit (negative binomial) 
## 当てはめたいモデル（負の二項回帰）を定義します。
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  ## 関心のあるアウトカムとしてのケースの数をセットします。
  case_int ~
    ## use epiweek to account for the trend
    ## トレンドの説明のためにepiwreekを使用します。
    epiweek +
    ## use the furier terms to account for seasonality
    ## 季節性の説明のためにフーリエ項を使用します。
    fourier + 
    ## add in whether in the pre- or post-period 
    ## 前あるいは後ろの期間であるかを追加します。
    intervention + 
    ## add in the time post intervention 
    ## 介入後の期間を追加します。
    time_post
    )

## fit your model using the counts dataset
## countsデータセットを使用して、モデルを当てはめます。
fitted_model <- trending::fit(model, counts)

## calculate confidence intervals and prediction intervals
## 信頼区間と予測区間を算出します。
observed <- predict(fitted_model, simulate_pi = FALSE)



## show estimates and percentage change in a table
## 推定値と変化率をテーブル内に表示します。
fitted_model %>% 
  ## extract original negative binomial regression
  ## 元の負の二項回帰を抽出します。
  get_model() %>% 
  ## get a tidy dataframe of results
  ## 結果についてのtidy なデータフレームを取得します。
  tidy(exponentiate = TRUE, 
       conf.int = TRUE) %>% 
  ## only keep the intervention value 
  ## 介入の値のみを保持します。
  filter(term == "intervention") %>% 
  ## change the IRR to percentage change for estimate and CIs 
  ## 推定値とCIsについて、IRRを変化率に変換します。
  mutate(
    ## for each of the columns of interest - create a new column
    ## 関心のあるそれぞれの列 - 新しい列を作成します。
    across(
      all_of(c("estimate", "conf.low", "conf.high")), 
      ## apply the formula to calculate percentage change
      ## 変化率を算出する公式を適用します。
            .f = function(i) 100 * (i - 1), 
      ## add a suffix to new column names with "_perc"
      ## "_perc" という名前の設備後を新たな列に追加します。
      .names = "{.col}_perc")
    ) %>% 
  ## only keep (and rename) certain columns 
  ## 特定の列のみを保持します（そして列名を変更します）。
  select("IRR" = estimate, 
         "95%CI low" = conf.low, 
         "95%CI high" = conf.high,
         "Percentage change" = estimate_perc, 
         "95%CI low (perc)" = conf.low_perc, 
         "95%CI high (perc)" = conf.high_perc,
         "p-value" = p.value)
```

As previously we can visualise the outputs of the regression. 
前述のように、回帰の出力を視覚化できます。

```{r plot_interrupted}

ggplot(observed, aes(x = epiweek)) + 
  ## add in observed case counts as a line
  ## 観測されたケースの数を追加します。
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## add in a line for the model estimate
  ## モデル推定値についての線を追加します。
  geom_line(aes(y = estimate, col = "Estimate")) + 
  ## add in a band for the prediction intervals 
  ## 予測区間のバンドを追加します。
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add vertical line and label to show where forecasting started
  ## 予測開始の場所を示すため、垂直線とラベルを追加します。
  geom_vline(
           xintercept = as.Date(intervention_week), 
           linetype = "dashed") + 
  annotate(geom = "text", 
           label = "Intervention", 
           x = intervention_week, 
           y = max(observed$upper_pi), 
           angle = 90, 
           vjust = 1
           ) + 
  ## define colours
  ## 色を定義します。
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Estimate" = "red")) + 
  ## make a traditional plot (with black axes and white background)
  ## 伝統的なプロット（軸が黒で背景が白）を作成します。
  theme_classic()

```


<!-- ======================================================= -->
## Resources {  }
## 資料

[forecasting: principles and practice textbook](https://otexts.com/fpp3/)  
[EPIET timeseries analysis case studies](https://github.com/EPIET/TimeSeriesAnalysis)  
[Penn State course](https://online.stat.psu.edu/stat510/lesson/1) 
[Surveillance package manuscript](https://www.jstatsoft.org/article/view/v070i10)





